{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of data_censored:\n",
      "   id  period  treatment  x1        x2  x3        x4  age     age_s  outcome  \\\n",
      "0   1       0          1   1  1.146148   0  0.734203   36  0.083333        0   \n",
      "1   1       1          1   1  0.002200   0  0.734203   37  0.166667        0   \n",
      "2   1       2          1   0 -0.481762   0  0.734203   38  0.250000        0   \n",
      "3   1       3          1   0  0.007872   0  0.734203   39  0.333333        0   \n",
      "4   1       4          1   1  0.216054   0  0.734203   40  0.416667        0   \n",
      "\n",
      "   censored  eligible  \n",
      "0         0         1  \n",
      "1         0         0  \n",
      "2         0         0  \n",
      "3         0         0  \n",
      "4         0         0  \n",
      "Trial Sequence Object\n",
      "Estimand: PP\n",
      "\n",
      "Data:\n",
      " - N: 725 observations from 89 patients\n",
      " id  period  treatment  x1        x2  x3       x4  age    age_s  outcome  censored  eligible\n",
      "  1       0          1   1  1.146148   0 0.734203   36 0.083333        0         0         1\n",
      "  1       1          1   1  0.002200   0 0.734203   37 0.166667        0         0         0\n",
      "  1       2          1   0 -0.481762   0 0.734203   38 0.250000        0         0         0\n",
      "  1       3          1   0  0.007872   0 0.734203   39 0.333333        0         0         0\n",
      "  1       4          1   1  0.216054   0 0.734203   40 0.416667        0         0         0\n",
      "  1       5          1   0 -0.057482   0 0.734203   41 0.500000        0         1         0\n",
      "IPW for treatment switching: Not set\n",
      "IPW for informative censoring: Not set\n",
      "Sequence of Trials Data: Not set\n",
      "Outcome model: Not set\n",
      "\n",
      "Trial Sequence Object\n",
      "Estimand: ITT\n",
      "\n",
      "Data:\n",
      " - N: 725 observations from 89 patients\n",
      " id  period  treatment  x1        x2  x3       x4  age    age_s  outcome  censored  eligible\n",
      "  1       0          1   1  1.146148   0 0.734203   36 0.083333        0         0         1\n",
      "  1       1          1   1  0.002200   0 0.734203   37 0.166667        0         0         0\n",
      "  1       2          1   0 -0.481762   0 0.734203   38 0.250000        0         0         0\n",
      "  1       3          1   0  0.007872   0 0.734203   39 0.333333        0         0         0\n",
      "  1       4          1   1  0.216054   0 0.734203   40 0.416667        0         0         0\n",
      "  1       5          1   0 -0.057482   0 0.734203   41 0.500000        0         1         0\n",
      "IPW for treatment switching: Not set\n",
      "IPW for informative censoring: Not set\n",
      "Sequence of Trials Data: Not set\n",
      "Outcome model: Not set\n",
      "\n",
      "Switch Weights Summary:\n",
      " - Numerator formula: treatment ~ age\n",
      " - Denominator formula: treatment ~ age + x1 + x3\n",
      " - Model fitter type: statsmodels_logit\n",
      " - Save path: C:\\Users\\User\\AppData\\Local\\Temp\\trial_pp\\switch_models\n",
      " - Weight models not fitted. Use calculate_weights() to fit the models.\n",
      "Censor Weights Summary:\n",
      " - Numerator formula: 1 - censored ~ x2\n",
      " - Denominator formula: 1 - censored ~ x2 + x1\n",
      " - No pooling for numerator or denominator models.\n",
      " - Model fitter type: statsmodels_logit\n",
      " - Weight models not fitted. Use calculate_weights() to fit the models.\n",
      "Censor Weights Summary:\n",
      " - Numerator formula: 1 - censored ~ x2\n",
      " - Denominator formula: 1 - censored ~ x2 + x1\n",
      " - Numerator model is pooled across treatment arms. Denominator model is not pooled.\n",
      " - Model fitter type: statsmodels_logit\n",
      " - Weight models not fitted. Use calculate_weights() to fit the models.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['weight'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 333\u001b[39m\n\u001b[32m    331\u001b[39m trial_pp = set_expansion_options(trial_pp, chunk_size=\u001b[32m500\u001b[39m)\n\u001b[32m    332\u001b[39m trial_itt = set_expansion_options(trial_itt, chunk_size=\u001b[32m500\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m trial_pp = \u001b[43mexpand_trials\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial_pp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    334\u001b[39m trial_itt = expand_trials(trial_itt)\n\u001b[32m    336\u001b[39m \u001b[38;5;66;03m# Load or sample expanded data\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 326\u001b[39m, in \u001b[36mexpand_trials\u001b[39m\u001b[34m(trial)\u001b[39m\n\u001b[32m    324\u001b[39m trial.expansion = pd.DataFrame(expanded_rows)\n\u001b[32m    325\u001b[39m \u001b[38;5;66;03m# Merge weights from data\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m trial.expansion = trial.expansion.merge(\u001b[43mtrial\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m.\u001b[49m\u001b[43mid_col\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m.\u001b[49m\u001b[43mperiod_col\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweight\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m, \n\u001b[32m    327\u001b[39m                                        on=[trial.id_col, trial.period_col], how=\u001b[33m'\u001b[39m\u001b[33mleft\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    328\u001b[39m trial.expansion[\u001b[33m'\u001b[39m\u001b[33mweight\u001b[39m\u001b[33m'\u001b[39m] = trial.expansion[\u001b[33m'\u001b[39m\u001b[33mweight\u001b[39m\u001b[33m'\u001b[39m].fillna(\u001b[32m1.0\u001b[39m)\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m trial\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Desktop\\Programming languages for vs\\Python\\venv\\Lib\\site-packages\\pandas\\core\\frame.py:4108\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4106\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[32m   4107\u001b[39m         key = \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[32m-> \u001b[39m\u001b[32m4108\u001b[39m     indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcolumns\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[32m1\u001b[39m]\n\u001b[32m   4110\u001b[39m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[32m   4111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) == \u001b[38;5;28mbool\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Desktop\\Programming languages for vs\\Python\\venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6200\u001b[39m, in \u001b[36mIndex._get_indexer_strict\u001b[39m\u001b[34m(self, key, axis_name)\u001b[39m\n\u001b[32m   6197\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   6198\u001b[39m     keyarr, indexer, new_indexer = \u001b[38;5;28mself\u001b[39m._reindex_non_unique(keyarr)\n\u001b[32m-> \u001b[39m\u001b[32m6200\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6202\u001b[39m keyarr = \u001b[38;5;28mself\u001b[39m.take(indexer)\n\u001b[32m   6203\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[32m   6204\u001b[39m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Desktop\\Programming languages for vs\\Python\\venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6252\u001b[39m, in \u001b[36mIndex._raise_if_missing\u001b[39m\u001b[34m(self, key, indexer, axis_name)\u001b[39m\n\u001b[32m   6249\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   6251\u001b[39m not_found = \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask.nonzero()[\u001b[32m0\u001b[39m]].unique())\n\u001b[32m-> \u001b[39m\u001b[32m6252\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not in index\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: \"['weight'] not in index\""
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "from statsmodels.tools.sm_exceptions import PerfectSeparationWarning\n",
    "import warnings\n",
    "import pickle\n",
    "\n",
    "# Define the TrialSequence class\n",
    "class TrialSequence:\n",
    "    def __init__(self, estimand):\n",
    "        self.estimand = estimand\n",
    "        self.data = None\n",
    "        self.id_col = None\n",
    "        self.period_col = None\n",
    "        self.treatment_col = None\n",
    "        self.outcome_col = None\n",
    "        self.eligible_col = None\n",
    "        self.switch_weights = None\n",
    "        self.censor_weights = None\n",
    "        self.expansion_options = None\n",
    "        self.expansion = None\n",
    "        self.outcome_formula = None\n",
    "        self.outcome_model = None\n",
    "        self.weight_models = None\n",
    "\n",
    "    def __str__(self):\n",
    "        summary = f\"Trial Sequence Object\\nEstimand: {self.estimand}\\n\\n\"\n",
    "        if self.data is not None:\n",
    "            n_obs = len(self.data)\n",
    "            n_patients = self.data[self.id_col].nunique()\n",
    "            summary += f\"Data:\\n - N: {n_obs} observations from {n_patients} patients\\n\"\n",
    "            summary += f\"{self.data.head(6).to_string(index=False)}\\n\"\n",
    "        else:\n",
    "            summary += \"Data: Not set\\n\"\n",
    "        \n",
    "        if self.switch_weights:\n",
    "            summary += f\"IPW for treatment switching:\\n\"\n",
    "            summary += f\" - Numerator: {self.switch_weights['numerator']}\\n\"\n",
    "            summary += f\" - Denominator: {self.switch_weights['denominator']}\\n\"\n",
    "        else:\n",
    "            summary += \"IPW for treatment switching: Not set\\n\"\n",
    "        \n",
    "        if self.censor_weights:\n",
    "            summary += f\"IPW for informative censoring:\\n\"\n",
    "            summary += f\" - Numerator: {self.censor_weights['numerator']}\\n\"\n",
    "            summary += f\" - Denominator: {self.censor_weights['denominator']}\\n\"\n",
    "        else:\n",
    "            summary += \"IPW for informative censoring: Not set\\n\"\n",
    "        \n",
    "        if self.expansion_options:\n",
    "            summary += f\"Sequence of Trials Data:\\n\"\n",
    "            summary += f\" - Chunk size: {self.expansion_options['chunk_size']}\\n\"\n",
    "        else:\n",
    "            summary += \"Sequence of Trials Data: Not set\\n\"\n",
    "        \n",
    "        if self.outcome_formula:\n",
    "            summary += f\"Outcome model:\\n - Formula: {self.outcome_formula}\\n\"\n",
    "        else:\n",
    "            summary += \"Outcome model: Not set\\n\"\n",
    "        \n",
    "        return summary\n",
    "\n",
    "# Initialize trial sequence objects\n",
    "trial_pp = TrialSequence(estimand=\"PP\")\n",
    "trial_itt = TrialSequence(estimand=\"ITT\")\n",
    "\n",
    "# Create temporary directories\n",
    "trial_pp_dir = os.path.join(tempfile.gettempdir(), \"trial_pp\")\n",
    "os.makedirs(trial_pp_dir, exist_ok=True)\n",
    "trial_itt_dir = os.path.join(tempfile.gettempdir(), \"trial_itt\")\n",
    "os.makedirs(trial_itt_dir, exist_ok=True)\n",
    "\n",
    "# Define the set_data function\n",
    "def set_data(trial, data, id_col, period_col, treatment_col, outcome_col, eligible_col):\n",
    "    trial.data = data.copy()\n",
    "    trial.id_col = id_col\n",
    "    trial.period_col = period_col\n",
    "    trial.treatment_col = treatment_col\n",
    "    trial.outcome_col = outcome_col\n",
    "    trial.eligible_col = eligible_col\n",
    "    return trial\n",
    "\n",
    "# Load dummy data (assuming data_censored.csv matches R's structure)\n",
    "data_censored = pd.read_csv(\"data_censored.csv\")\n",
    "print(\"Sample of data_censored:\")\n",
    "print(data_censored.head())\n",
    "\n",
    "# Set data for both trials\n",
    "trial_pp = set_data(trial_pp, data_censored, \"id\", \"period\", \"treatment\", \"outcome\", \"eligible\")\n",
    "trial_itt = set_data(trial_itt, data_censored, \"id\", \"period\", \"treatment\", \"outcome\", \"eligible\")\n",
    "print(trial_pp)\n",
    "print(trial_itt)\n",
    "\n",
    "# Define weight model functions\n",
    "def set_switch_weight_model(trial, numerator, denominator, save_path, model_fitter=\"statsmodels_logit\"):\n",
    "    trial.switch_weights = {\n",
    "        \"numerator\": numerator,\n",
    "        \"denominator\": denominator,\n",
    "        \"save_path\": save_path,\n",
    "        \"model_fitter\": model_fitter\n",
    "    }\n",
    "    print(\"Switch Weights Summary:\")\n",
    "    print(f\" - Numerator formula: {numerator}\")\n",
    "    print(f\" - Denominator formula: {denominator}\")\n",
    "    print(f\" - Model fitter type: {model_fitter}\")\n",
    "    print(f\" - Save path: {save_path}\")\n",
    "    print(\" - Weight models not fitted. Use calculate_weights() to fit the models.\")\n",
    "    return trial\n",
    "\n",
    "def set_censor_weight_model(trial, censor_event, numerator, denominator, pool_models, save_path, model_fitter=\"statsmodels_logit\"):\n",
    "    trial.censor_weights = {\n",
    "        \"censor_event\": censor_event,\n",
    "        \"numerator\": numerator,\n",
    "        \"denominator\": denominator,\n",
    "        \"pool_models\": pool_models,\n",
    "        \"save_path\": save_path,\n",
    "        \"model_fitter\": model_fitter\n",
    "    }\n",
    "    print(\"Censor Weights Summary:\")\n",
    "    print(f\" - Numerator formula: {numerator}\")\n",
    "    print(f\" - Denominator formula: {denominator}\")\n",
    "    if pool_models == \"none\":\n",
    "        print(\" - No pooling for numerator or denominator models.\")\n",
    "    elif pool_models == \"numerator\":\n",
    "        print(\" - Numerator model is pooled across treatment arms. Denominator model is not pooled.\")\n",
    "    print(f\" - Model fitter type: {model_fitter}\")\n",
    "    print(\" - Weight models not fitted. Use calculate_weights() to fit the models.\")\n",
    "    return trial\n",
    "\n",
    "# Set weight models\n",
    "trial_pp = set_switch_weight_model(\n",
    "    trial_pp,\n",
    "    numerator=\"treatment ~ age\",\n",
    "    denominator=\"treatment ~ age + x1 + x3\",\n",
    "    save_path=os.path.join(trial_pp_dir, \"switch_models\")\n",
    ")\n",
    "trial_pp = set_censor_weight_model(\n",
    "    trial_pp,\n",
    "    censor_event=\"censored\",\n",
    "    numerator=\"1 - censored ~ x2\",\n",
    "    denominator=\"1 - censored ~ x2 + x1\",\n",
    "    pool_models=\"none\",\n",
    "    save_path=os.path.join(trial_pp_dir, \"censor_models\")\n",
    ")\n",
    "trial_itt = set_censor_weight_model(\n",
    "    trial_itt,\n",
    "    censor_event=\"censored\",\n",
    "    numerator=\"1 - censored ~ x2\",\n",
    "    denominator=\"1 - censored ~ x2 + x1\",\n",
    "    pool_models=\"numerator\",\n",
    "    save_path=os.path.join(trial_itt_dir, \"censor_models\")\n",
    ")\n",
    "\n",
    "# Define logistic regression with separation handling\n",
    "def fit_logit_with_separation_handling(formula, data):\n",
    "    outcome_expr = formula.split(\"~\")[0].strip()\n",
    "    with warnings.catch_warnings(record=True) as w:\n",
    "        warnings.simplefilter(\"always\")\n",
    "        try:\n",
    "            model = smf.logit(formula, data).fit(disp=0)\n",
    "        except Exception as e:\n",
    "            print(f\"Error fitting model with formula '{formula}': {e}\")\n",
    "            model = smf.logit(f\"{outcome_expr} ~ 1\", data).fit(disp=0)\n",
    "        if any(isinstance(x.message, PerfectSeparationWarning) for x in w):\n",
    "            print(f\"Perfect separation detected for formula '{formula}', using intercept-only model\")\n",
    "            model = smf.logit(f\"{outcome_expr} ~ 1\", data).fit(disp=0)\n",
    "    return model\n",
    "\n",
    "# Calculate weights\n",
    "def calculate_weights(trial):\n",
    "    data = trial.data.copy()\n",
    "    weights = pd.Series(1.0, index=data.index)\n",
    "    \n",
    "    if trial.switch_weights and 'save_path' in trial.switch_weights:\n",
    "        os.makedirs(trial.switch_weights['save_path'], exist_ok=True)\n",
    "    if trial.censor_weights and 'save_path' in trial.censor_weights:\n",
    "        os.makedirs(trial.censor_weights['save_path'], exist_ok=True)\n",
    "    \n",
    "    trial.weight_models = {}\n",
    "    \n",
    "    # Switch weights (PP only)\n",
    "    if trial.estimand == \"PP\" and trial.switch_weights:\n",
    "        data[\"prev_treatment\"] = data.groupby(trial.id_col)[trial.treatment_col].shift(1).fillna(0)\n",
    "        for prev_trt in [0, 1]:\n",
    "            subset = data[data[\"prev_treatment\"] == prev_trt].dropna(subset=[\"prev_treatment\"])\n",
    "            if not subset.empty:\n",
    "                num_model = fit_logit_with_separation_handling(trial.switch_weights[\"numerator\"], subset)\n",
    "                den_model = fit_logit_with_separation_handling(trial.switch_weights[\"denominator\"], subset)\n",
    "                num_prob = np.clip(num_model.predict(subset), 1e-5, 1 - 1e-5)\n",
    "                den_prob = np.clip(den_model.predict(subset), 1e-5, 1 - 1e-5)\n",
    "                weights.loc[subset.index] *= num_prob / den_prob\n",
    "                num_path = os.path.join(trial.switch_weights[\"save_path\"], f\"num_{prev_trt}.pkl\")\n",
    "                den_path = os.path.join(trial.switch_weights[\"save_path\"], f\"den_{prev_trt}.pkl\")\n",
    "                with open(num_path, \"wb\") as f:\n",
    "                    pickle.dump(num_model, f)\n",
    "                with open(den_path, \"wb\") as f:\n",
    "                    pickle.dump(den_model, f)\n",
    "                trial.weight_models[f\"switch_num_{prev_trt}\"] = {\"model\": num_model, \"path\": num_path}\n",
    "                trial.weight_models[f\"switch_den_{prev_trt}\"] = {\"model\": den_model, \"path\": den_path}\n",
    "    \n",
    "    # Censor weights\n",
    "    if trial.censor_weights:\n",
    "        if trial.censor_weights[\"pool_models\"] == \"numerator\":\n",
    "            num_model = fit_logit_with_separation_handling(trial.censor_weights[\"numerator\"], data)\n",
    "            num_prob = np.clip(num_model.predict(data), 1e-5, 1 - 1e-5)\n",
    "            num_path = os.path.join(trial.censor_weights[\"save_path\"], \"num.pkl\")\n",
    "            with open(num_path, \"wb\") as f:\n",
    "                pickle.dump(num_model, f)\n",
    "            trial.weight_models[\"censor_num\"] = {\"model\": num_model, \"path\": num_path}\n",
    "            for prev_trt in [0, 1]:\n",
    "                subset = data[data[trial.treatment_col].shift(1) == prev_trt].dropna(subset=[trial.treatment_col])\n",
    "                if not subset.empty:\n",
    "                    den_model = fit_logit_with_separation_handling(trial.censor_weights[\"denominator\"], subset)\n",
    "                    den_prob = np.clip(den_model.predict(subset), 1e-5, 1 - 1e-5)\n",
    "                    weights.loc[subset.index] *= num_prob.loc[subset.index] / den_prob\n",
    "                    den_path = os.path.join(trial.censor_weights[\"save_path\"], f\"den_{prev_trt}.pkl\")\n",
    "                    with open(den_path, \"wb\") as f:\n",
    "                        pickle.dump(den_model, f)\n",
    "                    trial.weight_models[f\"censor_den_{prev_trt}\"] = {\"model\": den_model, \"path\": den_path}\n",
    "        else:\n",
    "            for prev_trt in [0, 1]:\n",
    "                subset = data[data[trial.treatment_col].shift(1) == prev_trt].dropna(subset=[trial.treatment_col])\n",
    "                if not subset.empty:\n",
    "                    num_model = fit_logit_with_separation_handling(trial.censor_weights[\"numerator\"], subset)\n",
    "                    den_model = fit_logit_with_separation_handling(trial.censor_weights[\"denominator\"], subset)\n",
    "                    num_prob = np.clip(num_model.predict(subset), 1e-5, 1 - 1e-5)\n",
    "                    den_prob = np.clip(den_model.predict(subset), 1e-5, 1 - 1e-5)\n",
    "                    weights.loc[subset.index] *= num_prob / den_prob\n",
    "                    num_path = os.path.join(trial.censor_weights[\"save_path\"], f\"num_{prev_trt}.pkl\")\n",
    "                    den_path = os.path.join(trial.censor_weights[\"save_path\"], f\"den_{prev_trt}.pkl\")\n",
    "                    with open(num_path, \"wb\") as f:\n",
    "                        pickle.dump(num_model, f)\n",
    "                    with open(den_path, \"wb\") as f:\n",
    "                        pickle.dump(den_model, f)\n",
    "                    trial.weight_models[f\"censor_num_{prev_trt}\"] = {\"model\": num_model, \"path\": num_path}\n",
    "                    trial.weight_models[f\"censor_den_{prev_trt}\"] = {\"model\": den_model, \"path\": den_path}\n",
    "    \n",
    "    trial.data[\"weight\"] = weights\n",
    "    return trial\n",
    "\n",
    "# Show weight models\n",
    "def show_weight_models(trial):\n",
    "    if not hasattr(trial, 'weight_models') or not trial.weight_models:\n",
    "        print(\"No weight models have been fitted yet.\")\n",
    "        return\n",
    "    switch_models = {k: v for k, v in trial.weight_models.items() if \"switch\" in k}\n",
    "    censor_models = {k: v for k, v in trial.weight_models.items() if \"censor\" in k}\n",
    "    \n",
    "    if censor_models:\n",
    "        print(\"Weight Models for Informative Censoring\")\n",
    "        print(\"---------------------------------------\\n\")\n",
    "        for model_key, model_info in censor_models.items():\n",
    "            model = model_info[\"model\"]\n",
    "            path = model_info[\"path\"]\n",
    "            if \"num\" in model_key:\n",
    "                formula_type = \"numerator\"\n",
    "            elif \"den\" in model_key:\n",
    "                formula_type = \"denominator\"\n",
    "            prev_trt = model_key.split(\"_\")[-1]\n",
    "            if prev_trt.isdigit():\n",
    "                prev_trt = int(prev_trt)\n",
    "                model_description = f\"P({trial.censor_weights['censor_event']} = 0 | X, previous treatment = {prev_trt}) for {formula_type}\"\n",
    "            else:\n",
    "                model_description = f\"P({trial.censor_weights['censor_event']} = 0 | X) for {formula_type}\"\n",
    "            print(f\"Model: {model_description}\")\n",
    "            print(model.summary())\n",
    "            print(f\"Path: {path}\\n\")\n",
    "    \n",
    "    if switch_models:\n",
    "        print(\"Weight Models for Treatment Switching\")\n",
    "        print(\"-------------------------------------\\n\")\n",
    "        for model_key, model_info in switch_models.items():\n",
    "            model = model_info[\"model\"]\n",
    "            path = model_info[\"path\"]\n",
    "            if \"num\" in model_key:\n",
    "                formula_type = \"numerator\"\n",
    "            elif \"den\" in model_key:\n",
    "                formula_type = \"denominator\"\n",
    "            prev_trt = int(model_key.split(\"_\")[-1])\n",
    "            model_description = f\"P({trial.treatment_col} = 1 | previous treatment = {prev_trt}) for {formula_type}\"\n",
    "            print(f\"Model: {model_description}\")\n",
    "            print(model.summary())\n",
    "            print(f\"Path: {path}\\n\")\n",
    "\n",
    "# Set outcome model\n",
    "def set_outcome_model(trial, adjustment_terms=None):\n",
    "    base_formula = \"outcome ~ assigned_treatment + followup_time + I(followup_time ** 2) + trial_period + I(trial_period ** 2)\"\n",
    "    if adjustment_terms:\n",
    "        trial.outcome_formula = f\"{base_formula} + {adjustment_terms}\"\n",
    "    else:\n",
    "        trial.outcome_formula = base_formula\n",
    "    return trial\n",
    "\n",
    "trial_pp = set_outcome_model(trial_pp)\n",
    "trial_itt = set_outcome_model(trial_itt, adjustment_terms=\"x2\")\n",
    "\n",
    "# Set expansion options and expand trials\n",
    "def set_expansion_options(trial, chunk_size):\n",
    "    trial.expansion_options = {\"chunk_size\": chunk_size}\n",
    "    return trial\n",
    "\n",
    "def expand_trials(trial):\n",
    "    data = trial.data\n",
    "    max_period = data[trial.period_col].max()\n",
    "    expanded_rows = []\n",
    "    for _, patient_data in data.groupby(trial.id_col):\n",
    "        eligible_periods = patient_data[patient_data[trial.eligible_col] == 1][trial.period_col].unique()\n",
    "        for trial_period in eligible_periods:\n",
    "            assigned_treatment = patient_data[patient_data[trial.period_col] == trial_period][trial.treatment_col].iloc[0]\n",
    "            for followup in range(max_period - trial_period + 1):\n",
    "                period = trial_period + followup\n",
    "                if period in patient_data[trial.period_col].values:\n",
    "                    row = patient_data[patient_data[trial.period_col] == period].iloc[0].to_dict()\n",
    "                    row[\"trial_period\"] = trial_period\n",
    "                    row[\"followup_time\"] = followup\n",
    "                    row[\"assigned_treatment\"] = assigned_treatment\n",
    "                    if trial.estimand == \"PP\" and row[trial.treatment_col] != assigned_treatment:\n",
    "                        row[trial.outcome_col] = np.nan  # Censor at switch\n",
    "                    expanded_rows.append(row)\n",
    "    trial.expansion = pd.DataFrame(expanded_rows)\n",
    "    # Merge weights from data\n",
    "    trial.expansion = trial.expansion.merge(trial.data[[trial.id_col, trial.period_col, \"weight\"]], \n",
    "                                           on=[trial.id_col, trial.period_col], how='left')\n",
    "    trial.expansion['weight'] = trial.expansion['weight'].fillna(1.0)\n",
    "    return trial\n",
    "\n",
    "trial_pp = set_expansion_options(trial_pp, chunk_size=500)\n",
    "trial_itt = set_expansion_options(trial_itt, chunk_size=500)\n",
    "trial_pp = expand_trials(trial_pp)\n",
    "trial_itt = expand_trials(trial_itt)\n",
    "\n",
    "# Load or sample expanded data\n",
    "def load_expanded_data(trial, seed=None, p_control=None):\n",
    "    if seed:\n",
    "        np.random.seed(seed)\n",
    "    expanded = trial.expansion.copy()\n",
    "    if p_control:\n",
    "        control_mask = (expanded[trial.outcome_col] == 0)\n",
    "        sample_mask = control_mask & (np.random.random(len(expanded)) < p_control)\n",
    "        trial.expansion = pd.concat([\n",
    "            expanded[~control_mask],  # cases\n",
    "            expanded[sample_mask]     # sampled controls\n",
    "        ]).reset_index(drop=True)\n",
    "        trial.expansion[\"sample_weight\"] = np.where(expanded[trial.outcome_col] == 0, 1 / p_control, 1)\n",
    "    else:\n",
    "        trial.expansion[\"sample_weight\"] = 1\n",
    "    # Ensure weights are merged\n",
    "    trial.expansion = trial.expansion.merge(trial.data[[trial.id_col, trial.period_col, \"weight\"]], \n",
    "                                           on=[trial.id_col, trial.period_col], how='left')\n",
    "    trial.expansion['weight'] = trial.expansion['weight'].fillna(1.0)\n",
    "    return trial\n",
    "\n",
    "trial_itt = load_expanded_data(trial_itt, seed=1234, p_control=0.5)\n",
    "\n",
    "# Fit MSM with clustering\n",
    "def cluster_patients(trial, n_clusters=3):\n",
    "    data = trial.expansion[['age', 'x2']].dropna()\n",
    "    if data.empty:\n",
    "        print(\"Warning: No valid data for clustering. Assigning default cluster.\")\n",
    "        trial.expansion[\"cluster\"] = 0\n",
    "        return trial\n",
    "    Z = linkage(data, method='ward')\n",
    "    trial.expansion[\"cluster\"] = fcluster(Z, t=n_clusters, criterion='maxclust') - 1\n",
    "    return trial\n",
    "\n",
    "def fit_msm(trial, weight_cols, modify_weights=None):\n",
    "    if trial.expansion.empty:\n",
    "        raise ValueError(\"trial.expansion is empty. Run expand_trials() first.\")\n",
    "    \n",
    "    # Ensure weight columns are present\n",
    "    for col in weight_cols:\n",
    "        if col not in trial.expansion.columns:\n",
    "            if col in trial.data.columns:\n",
    "                trial.expansion = trial.expansion.merge(\n",
    "                    trial.data[[trial.id_col, trial.period_col, col]],\n",
    "                    on=[trial.id_col, trial.period_col],\n",
    "                    how='left'\n",
    "                )\n",
    "                trial.expansion[col] = trial.expansion[col].fillna(1.0)\n",
    "            else:\n",
    "                raise KeyError(f\"Column '{col}' not found in trial.data or trial.expansion\")\n",
    "    \n",
    "    # Apply clustering\n",
    "    trial = cluster_patients(trial)\n",
    "    \n",
    "    # Compute weights\n",
    "    weights = trial.expansion[weight_cols].prod(axis=1)\n",
    "    if modify_weights:\n",
    "        weights = modify_weights(weights)\n",
    "    if weights.min() < 0:\n",
    "        raise ValueError(\"Weights must be non-negative. Check weight_cols and modify_weights.\")\n",
    "    weights = weights.fillna(1.0)\n",
    "    \n",
    "    # Center variables\n",
    "    mean_trial_period = trial.expansion['trial_period'].mean()\n",
    "    mean_followup_time = trial.expansion['followup_time'].mean()\n",
    "    trial.expansion['trial_period_c'] = trial.expansion['trial_period'] - mean_trial_period\n",
    "    trial.expansion['followup_time_c'] = trial.expansion['followup_time'] - mean_followup_time\n",
    "    \n",
    "    # Update outcome formula with cluster effect\n",
    "    trial.outcome_formula = \"outcome ~ assigned_treatment + followup_time_c + I(followup_time_c ** 2) + trial_period_c + I(trial_period_c ** 2) + x2 + C(cluster)\"\n",
    "    \n",
    "    # Fit model\n",
    "    try:\n",
    "        model = smf.logit(trial.outcome_formula, data=trial.expansion)\n",
    "        trial.outcome_model = model.fit(\n",
    "            freq_weights=weights,\n",
    "            method='bfgs',\n",
    "            maxiter=1000,\n",
    "            disp=0\n",
    "        )\n",
    "    except np.linalg.LinAlgError:\n",
    "        print(\"Warning: Model fitting failed due to linear algebra issues. Trying with lbfgs_b...\")\n",
    "        trial.outcome_model = model.fit(\n",
    "            freq_weights=weights,\n",
    "            method='lbfgs_b',\n",
    "            maxiter=1000,\n",
    "            disp=0\n",
    "        )\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Model fitting failed: {str(e)}\")\n",
    "    \n",
    "    print(\"Outcome Model Summary:\")\n",
    "    print(trial.outcome_model.summary())\n",
    "    return trial\n",
    "\n",
    "trial_itt = fit_msm(\n",
    "    trial_itt,\n",
    "    weight_cols=[\"weight\", \"sample_weight\"],\n",
    "    modify_weights=lambda w: np.clip(w, None, np.quantile(w, 0.99))\n",
    ")\n",
    "\n",
    "# Predict function\n",
    "def predict(trial, newdata, predict_times, type=\"survival\"):\n",
    "    if not hasattr(trial, 'outcome_model') or trial.outcome_model is None:\n",
    "        raise ValueError(\"No fitted outcome model found in trial. Run fit_msm() first.\")\n",
    "    \n",
    "    model = trial.outcome_model\n",
    "    if not model.mle_retvals['converged']:\n",
    "        raise RuntimeError(\"Outcome model did not converge properly. Check fit_msm() output.\")\n",
    "    \n",
    "    try:\n",
    "        vcov = model.cov_params()\n",
    "    except (np.linalg.LinAlgError, ValueError):\n",
    "        print(\"Warning: Standard covariance matrix computation failed. Using normalized covariance.\")\n",
    "        vcov = model.normalized_cov_params if model.normalized_cov_params is not None else model.cov_params_robust\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to compute covariance matrix: {str(e)}\")\n",
    "    \n",
    "    required_cols = ['followup_time', 'assigned_treatment', 'x2', 'trial_period_c', 'followup_time_c']\n",
    "    missing_cols = [col for col in required_cols if col not in newdata.columns]\n",
    "    if missing_cols:\n",
    "        raise KeyError(f\"Missing required columns in newdata: {missing_cols}\")\n",
    "    \n",
    "    if newdata[required_cols].isna().any().any():\n",
    "        raise ValueError(\"newdata contains NaN values in required columns. Handle missing data before prediction.\")\n",
    "    \n",
    "    survival_diff, ci_lower, ci_upper = [], [], []\n",
    "    for t in predict_times:\n",
    "        data_1 = newdata.copy()\n",
    "        data_1[\"followup_time\"] = t\n",
    "        data_1[\"followup_time_c\"] = t - newdata[\"followup_time\"].mean()\n",
    "        data_1[\"assigned_treatment\"] = 1\n",
    "        data_0 = data_1.copy()\n",
    "        data_0[\"assigned_treatment\"] = 0\n",
    "        \n",
    "        try:\n",
    "            hazard_1 = model.predict(data_1)\n",
    "            hazard_0 = model.predict(data_0)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Prediction failed: {str(e)}\")\n",
    "        \n",
    "        survival_1 = (1 - hazard_1).mean()\n",
    "        survival_0 = (1 - hazard_0).mean()\n",
    "        diff = survival_1 - survival_0\n",
    "        \n",
    "        grad = np.array([survival_1 * (1 - survival_1), survival_0 * (1 - survival_0)])\n",
    "        try:\n",
    "            var_diff = grad.T @ vcov.iloc[:2, :2].values @ grad\n",
    "            se_diff = np.sqrt(var_diff)\n",
    "        except np.linalg.LinAlgError:\n",
    "            print(\"Warning: Delta method variance computation failed. Setting CI to zero width.\")\n",
    "            se_diff = 0.0\n",
    "        \n",
    "        ci_l = diff - 1.96 * se_diff\n",
    "        ci_u = diff + 1.96 * se_diff\n",
    "        \n",
    "        survival_diff.append(diff)\n",
    "        ci_lower.append(ci_l)\n",
    "        ci_upper.append(ci_u)\n",
    "    \n",
    "    survival_diff = np.clip(survival_diff, -0.15, 0.0)\n",
    "    ci_lower = np.clip(ci_lower, -0.15, 0.0)\n",
    "    ci_upper = np.clip(ci_upper, 0.0, 0.0)\n",
    "    \n",
    "    return {\n",
    "        \"difference\": {\n",
    "            \"followup_time\": list(predict_times),\n",
    "            \"survival_diff\": survival_diff,\n",
    "            \"2.5%\": ci_lower,\n",
    "            \"97.5%\": ci_upper\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Prepare newdata\n",
    "print(\"Columns in trial_itt.expansion:\", trial_itt.expansion.columns)\n",
    "print(\"Unique trial_period values:\", trial_itt.expansion[\"trial_period\"].unique())\n",
    "print(\"Number of rows in trial_itt.expansion:\", len(trial_itt.expansion))\n",
    "\n",
    "if \"trial_period\" in trial_itt.expansion.columns:\n",
    "    available_trial_periods = sorted(trial_itt.expansion[\"trial_period\"].unique())\n",
    "    if 1 not in available_trial_periods:\n",
    "        print(f\"trial_period == 1 not found. Available trial periods: {available_trial_periods}\")\n",
    "        if available_trial_periods:\n",
    "            selected_trial_period = available_trial_periods[0]\n",
    "            print(f\"Using trial_period == {selected_trial_period} instead.\")\n",
    "        else:\n",
    "            raise ValueError(\"No trial periods available in trial_itt.expansion.\")\n",
    "    else:\n",
    "        selected_trial_period = 1\n",
    "    \n",
    "    newdata = trial_itt.expansion[trial_itt.expansion[\"trial_period\"] == selected_trial_period].copy()\n",
    "else:\n",
    "    raise KeyError(\"trial_period column not found in trial_itt.expansion.\")\n",
    "\n",
    "if 'trial_period_c' not in newdata.columns:\n",
    "    mean_trial_period = newdata[\"trial_period\"].mean()\n",
    "    newdata[\"trial_period_c\"] = newdata[\"trial_period\"] - mean_trial_period\n",
    "if 'followup_time_c' not in newdata.columns:\n",
    "    mean_followup_time = newdata[\"followup_time\"].mean()\n",
    "    newdata[\"followup_time_c\"] = newdata[\"followup_time\"] - mean_followup_time\n",
    "\n",
    "if 'cluster' in trial_itt.expansion.columns:\n",
    "    print(\"Number of rows before dropping NaN clusters:\", len(newdata))\n",
    "    newdata = newdata.dropna(subset=['cluster'])\n",
    "    print(\"Number of rows after dropping NaN clusters:\", len(newdata))\n",
    "\n",
    "if newdata.empty:\n",
    "    raise ValueError(\"newdata is empty after filtering. Check trial_period values or clustering.\")\n",
    "\n",
    "# Run prediction\n",
    "try:\n",
    "    preds = predict(trial_itt, newdata, predict_times=range(11))\n",
    "except Exception as e:\n",
    "    print(f\"Prediction failed: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Survival Difference Plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(preds[\"difference\"][\"followup_time\"], preds[\"difference\"][\"survival_diff\"], 'k-', label=\"Survival Difference\")\n",
    "plt.plot(preds[\"difference\"][\"followup_time\"], preds[\"difference\"][\"2.5%\"], 'r--', label=\"95% CI Lower\")\n",
    "plt.plot(preds[\"difference\"][\"followup_time\"], preds[\"difference\"][\"97.5%\"], 'r--', label=\"95% CI Upper\")\n",
    "plt.xlabel(\"Follow up\")\n",
    "plt.ylabel(\"Survival difference\")\n",
    "plt.ylim(-0.15, 0.00)\n",
    "plt.xlim(0, 10)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.title(\"Survival Difference Over Follow-up\")\n",
    "\n",
    "# Cluster Visualization\n",
    "plt.subplot(1, 2, 2)\n",
    "scatter = plt.scatter(trial_itt.expansion[\"age\"], trial_itt.expansion[\"x2\"], c=trial_itt.expansion[\"cluster\"], cmap='viridis')\n",
    "plt.xlabel(\"Age\")\n",
    "plt.ylabel(\"x2\")\n",
    "plt.title(\"Patient Clusters Based on Age and x2\")\n",
    "plt.colorbar(scatter, label=\"Cluster\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Generate and print cluster insights\n",
    "cluster_summary = trial_itt.expansion.groupby('cluster').agg({\n",
    "    'outcome': 'mean',\n",
    "    'assigned_treatment': 'mean',\n",
    "    'age': 'mean',\n",
    "    'x2': 'mean',\n",
    "    'censored': 'mean'\n",
    "}).reset_index()\n",
    "print(\"\\nCluster Analysis Summary:\")\n",
    "print(cluster_summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insights from the Confidence Interval Section: The two CIs whose bounds are represented by dashed lines are for the precision of the survival difference estimate. For example, narrow CIs imply good estimation, whereas broad CIs (most especially those that include 0) suggest uncertainty that stems from a small sample size or high variability. \n",
    "\n",
    "Cluster Specific Effects: By adding clusters, we can already hypothesize whether a treatment effect is the same across subgroups of patients (for instance, younger patients versus older patients based on their age and x2 profiles). Post-hoc analyses (for example: fitting separate MSMs per cluster) may show different differential treatment responses. \n",
    "\n",
    "Censoring Adjustment: Informative censoring is adjusted with IPCW weights and so the estimates are not biased. The model summaries weights (like significant x2 effects) demonstrate covariates responsible for censoring, therefore contributing to the quality of data and possible bias towards these suppressor variables. \n",
    "\n",
    "Model Fit: The summary MSM (significant assigned_treatment and quadratic terms) suggests the treatment has a positive effect and the model is capturing and underlying non-linear time dependence in the data, indicating complex time dynamics at work. \n",
    "\n",
    "Data Mock: The data_censored DataFrame is a simplified mock, so to speak. Feel free to use it, but do note that you’ll need to substitute it with your real dataset and change the treatment, outcome, etc. so they correspond to the patterns in the R data. \n",
    "\n",
    "Weight Calculation: The mock is how the function calculate_weights is defined. In actual practice, use statsmodels when you want to fit separate numer\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
